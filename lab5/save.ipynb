{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size: int, output_size: int, activation: ActivationFunction=Identity(), random_seed: int=17):\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)  # в колабе распространяется только на текущую ячейку\n",
    "        self.weights = np.random.uniform(-1, 1, size=(input_size, output_size))\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.last_x = None\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # check dimensionality\n",
    "        batch_size = inputs.shape[0]\n",
    "        if inputs.shape != (batch_size, self.input_size):\n",
    "            raise ValueError(f'Unappropriate size of input. Expected: {[batch_size, self.input_size]} actual: {inputs.shape}')\n",
    "        # add biases\n",
    "        # inputs = np.hstack([inputs, np.ones((batch_size, 1))])\n",
    "        # record input to class\n",
    "        self.last_x = inputs\n",
    "        \n",
    "        # make outputs\n",
    "        y = inputs @ self.weights\n",
    "        self.last_y = y\n",
    "        outputs = self.activation(y)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, grad_outputs: np.ndarray, learning_rate: int=0.01) -> np.ndarray:\n",
    "        if self.last_x is None:\n",
    "            raise ValueError('try to call backward before forward')\n",
    "        grad_activation = self.activation.grad(self.last_y)\n",
    "        dZ = grad_outputs * grad_activation\n",
    "        dX = dZ @ self.weights.T\n",
    "        dW = self.last_x.T @ grad_outputs\n",
    "        self.weights -= learning_rate * dW\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, delta_1: np.ndarray, learning_rate: int=0.01) -> np.ndarray:\n",
    "        if self.last_x is None:\n",
    "            raise ValueError('try to call backward before forward')\n",
    "        \n",
    "        grad_activation = self.activation.grad(self.last_7)\n",
    "        delta_0 =  (delta_1 @ self.weights.T) * grad_activation\n",
    "        dW = self.last_x.T @ delta_1\n",
    "        self.weights -= learning_rate * dW\n",
    "        return delta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size: int, output_size: int, activation: ActivationFunction=Identity(), random_seed: int=17):\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)  # в колабе распространяется только на текущую ячейку\n",
    "        self.weights = np.random.uniform(-1, 1, size=(input_size, output_size))\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.last_x = None\n",
    "        self.activation = activation\n",
    "        self.biases = np.random.uniform(-1, 1, size=(output_size,))\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # check dimensionality\n",
    "        batch_size = inputs.shape[0]\n",
    "        if inputs.shape != (batch_size, self.input_size):\n",
    "            raise ValueError(f'Unappropriate size of input. Expected: {[batch_size, self.input_size]} actual: {inputs.shape}')\n",
    "        # add biases\n",
    "        # inputs = np.hstack([inputs, np.ones((batch_size, 1))])\n",
    "        # record input to class\n",
    "        self.last_x = inputs\n",
    "        \n",
    "        # make outputs\n",
    "        y = inputs @ self.weights + self.biases\n",
    "        self.last_y = y\n",
    "        outputs = self.activation(y)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray, learning_rate: int=0.01) -> np.ndarray:\n",
    "        if self.last_x is None:\n",
    "            raise ValueError('try to call backward before forward')\n",
    "        \n",
    "        grad_activation = self.activation.grad(self.last_y)\n",
    "        dldz = grad_output * grad_activation\n",
    "        dldx = dldz @ self.weights.T\n",
    "        dldw = self.last_x.T @ dldz\n",
    "        dldb = np.sum(dldz, axis=0)\n",
    "        self.weights -= learning_rate * dldw\n",
    "        self.biases -= learning_rate * dldb\n",
    "        return dldx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size: int, output_size: int, activation: ActivationFunction = Identity(), random_seed: int = 17):\n",
    "        np.random.seed(random_seed)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros(output_size)\n",
    "        self.inputs = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        self.inputs = inputs  # (batch_size, input_size)\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases  # (batch_size, output_size)\n",
    "        self.a = self.activation(self.z)  # (batch_size, output_size)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray, learning_rate: float = 0.01) -> np.ndarray:\n",
    "        # 1. Градиент по выходам (∂L/∂Z)\n",
    "        grad_z = grad_output * self.activation.grad(self.z)  # (batch_size, output_size)\n",
    "        \n",
    "        # 2. Градиент по входам (∂L/∂X)\n",
    "        grad_input = np.dot(grad_z, self.weights.T)  # (batch_size, input_size)\n",
    "        \n",
    "        # 3. Градиент по весам (∂L/∂W)\n",
    "        grad_weights = np.dot(self.inputs.T, grad_z)  # (input_size, output_size)\n",
    "        \n",
    "        # 4. Градиент по смещениям (∂L/∂b)\n",
    "        grad_biases = np.sum(grad_z, axis=0)  # (output_size,)\n",
    "        \n",
    "        # 5. Обновление параметров\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.biases -= learning_rate * grad_biases\n",
    "        \n",
    "        # 6. Возврат градиента по входам для использования в предыдущем слое\n",
    "        return grad_input"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
